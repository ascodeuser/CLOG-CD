{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ca8d5cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T12:19:38.726713Z",
     "iopub.status.busy": "2023-04-30T12:19:38.725752Z",
     "iopub.status.idle": "2023-04-30T12:19:48.575362Z",
     "shell.execute_reply": "2023-04-30T12:19:48.574134Z",
     "shell.execute_reply.started": "2023-04-30T12:19:38.726626Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import os\n",
    "import glob as gb\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import load_model\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Input, Add, Dropout, Activation,\\\n",
    "               BatchNormalization, Conv2D, MaxPooling2D, GlobalMaxPooling2D,MaxPool2D\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from mlxtend.plotting import plot_confusion_matrix                  # pip install mlxtend\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score \n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from operator import truediv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba2c26a",
   "metadata": {},
   "source": [
    "## Import and prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5b89b00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T12:22:26.931962Z",
     "iopub.status.busy": "2023-04-30T12:22:26.930512Z",
     "iopub.status.idle": "2023-04-30T12:22:26.943292Z",
     "shell.execute_reply": "2023-04-30T12:22:26.941890Z",
     "shell.execute_reply.started": "2023-04-30T12:22:26.931917Z"
    }
   },
   "outputs": [],
   "source": [
    "def image_datagenerator(trainpath, img_height=224, img_width=224, batch_size=20):\n",
    "    \n",
    "    train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2) # set validation split\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    \n",
    "    print(\"The data is being split into training and validation set\")\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "    trainpath,# This is the target directory\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True,\n",
    "    subset='training') # set as training data\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "\n",
    "    validation_generator = train_datagen.flow_from_directory(\n",
    "    trainpath, # same directory as training data\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True,\n",
    "    subset='validation') # set as validation data\n",
    "\n",
    "    # check the number of images in each class in the training dataset\n",
    "    No_images_per_class = []\n",
    "    training_class = []\n",
    "\n",
    "    for i in os.listdir (trainpath):         \n",
    "        Class_name = os.listdir(os.path.join(trainpath, i))\n",
    "        No_images_per_class.append(len(Class_name))\n",
    "        training_class.append(i)\n",
    "        print('Number of images in {} = {} \\n'.format(i, len(Class_name)))\n",
    "        \n",
    "    \n",
    "    return train_generator, validation_generator, training_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58dfc65",
   "metadata": {},
   "source": [
    "### Import the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea712196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 615 images belonging to 3 classes.\n",
      "Number of images in glioma = 286 \n",
      "\n",
      "Number of images in meningioma = 143 \n",
      "\n",
      "Number of images in pituitary tumor = 186 \n",
      "\n",
      "test_classes:  {'glioma': 0, 'meningioma': 1, 'pituitary tumor': 2}\n"
     ]
    }
   ],
   "source": [
    "# Initialize the ImageDataGenerator to rescale pixel values to the range [0, 1]\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Set image dimensions for resizing\n",
    "img_height = 224  # Height of the image after resizing\n",
    "img_width = 224   # Width of the image after resizing\n",
    "\n",
    "# Define the path to the test dataset\n",
    "testpath= ('V:/......../')\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        testpath,# This is the target directory\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=1,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False, \n",
    "        seed=42) \n",
    "\n",
    "\n",
    "# check the number of images in each class in the test dataset\n",
    "No_images_per_class = []\n",
    "test_class = []\n",
    "\n",
    "for i in os.listdir (testpath):       \n",
    "    Class_name = os.listdir(os.path.join(testpath, i))\n",
    "    No_images_per_class.append(len(Class_name))\n",
    "    test_class.append(i)\n",
    "    print('Number of images in {} = {} \\n'.format(i, len(Class_name)))\n",
    "        \n",
    "test_classes = test_generator.class_indices\n",
    "print('test_classes: ',test_classes)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b365cef4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T12:23:01.382477Z",
     "iopub.status.busy": "2023-04-30T12:23:01.381299Z",
     "iopub.status.idle": "2023-04-30T12:23:01.393921Z",
     "shell.execute_reply": "2023-04-30T12:23:01.392525Z",
     "shell.execute_reply.started": "2023-04-30T12:23:01.382422Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This class allows the user to choose and run different training processes for the model.\n",
    "It includes options for:\n",
    "1. Pretrained-ImageNet training\n",
    "2. DEG process training\n",
    "3. CLOG-CD with Δ=1, Δ=2, and Δ=4 training\n",
    "\n",
    "The user is prompted to select one of the options, and based on their input, the corresponding\n",
    "training function is executed. Each function trains the model using different configurations \n",
    "and processes, with varying levels and delta values.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class The_proccess():\n",
    "\n",
    "    def first(self):\n",
    "        print(\"Training the dataset based on a pretrained-imagenet\")\n",
    "        Level=-1\n",
    "        ImageNet_model(Dataset_path, g, index, Level, Process= 'pretrained-imagenet')\n",
    "        \n",
    "    def second(self):\n",
    "        print(\"Training ASG Model\")\n",
    "        Level=-1\n",
    "        delta= -1\n",
    "        ASG(g,index, Level, delta, Process='ASG')\n",
    "        \n",
    "    def third(self):\n",
    "        print(\"Training DEG Model\")\n",
    "        Level=0\n",
    "        delta=1\n",
    "        DEG(g,index, Level, delta, Process='DEG')\n",
    "\n",
    "    def forth(self):\n",
    "        print(\"Training CLOG-CD based on (\\u0394=1) process\")\n",
    "        Level=0\n",
    "        delta=1\n",
    "        CLOG_CD (g, index, Level, delta, Process='CLOG_CD (\\u0394=1)')\n",
    "\n",
    "    def fifth(self):\n",
    "        print(\"Training CLOG-CD based on (\\u0394=2) process\")\n",
    "        Level=0\n",
    "        delta=2\n",
    "        CLOG_CD (g, index, Level, delta, Process='CLOG_CD (\\u0394=2)')\n",
    "        \n",
    "    def sixth(self):\n",
    "        print(\"Training CLOG-CD based on (\\u0394=4) process\")\n",
    "        Level=0\n",
    "        delta=4\n",
    "        CLOG_CD (g, index, Level, delta, Process='CLOG_CD (\\u0394=4)')\n",
    "        \n",
    "   \n",
    "        \n",
    "    def __init__(self):\n",
    "        self.method = input(\"Which process do you want to use? \\n\\n 1) Traditional transfer learning. \\n\\n 2) ASG. \\n\\n 3) DEG. \\n\\n 4) CLOG_CD (\\u0394=1). \\n\\n 5) CLOG_CD (\\u0394=2). \\n\\n 6) CLOG_CD (\\u0394=4).  \\n\\n Please enter the corresponding number and hit enter >>>>> \")\n",
    "\n",
    "        if self.method == str(1):\n",
    "            self.first()\n",
    "        elif self.method == str(2):\n",
    "            self.second()\n",
    "        elif self.method == str(3):\n",
    "            self.third()\n",
    "        elif self.method == str(4):\n",
    "            self.forth()\n",
    "        elif self.method == str(5):\n",
    "            self.fifth()\n",
    "        elif self.method == str(6):\n",
    "            self.sixth()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7591568",
   "metadata": {},
   "source": [
    "### Traditional transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94b170a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T12:23:04.561392Z",
     "iopub.status.busy": "2023-04-30T12:23:04.561006Z",
     "iopub.status.idle": "2023-04-30T12:23:04.569710Z",
     "shell.execute_reply": "2023-04-30T12:23:04.568319Z",
     "shell.execute_reply.started": "2023-04-30T12:23:04.561357Z"
    }
   },
   "outputs": [],
   "source": [
    "def ImageNet_model(Dataset_path, granularity,index,Level, Process):       \n",
    "    \n",
    "    \n",
    "    folder= granularity[Level]  #'k_1'    #lowest_level or folder\n",
    "    index= index[Level]\n",
    "    \n",
    "    granularity_path= os.path.join(Dataset_path, folder)\n",
    "    train_generator, validation_generator, training_class = image_datagenerator(granularity_path)\n",
    "    \n",
    "\n",
    "    # loading ResNet50\n",
    "    # Loading the pretrained model without the output of the last convolution block \n",
    "    base_model = ResNet50(include_top=False, input_shape=(224, 224, 3), weights = 'imagenet')\n",
    "\n",
    "    # Flatten the output layer to 1 dimension\n",
    "    x = layers.Flatten()(base_model.output)\n",
    "    # Add a fully connected layer with 2048 hidden units and ReLU activation\n",
    "    x = layers.Dense(2048, activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    \n",
    "    # Adding a fully connected layer having len(image_class) neurons which will  give the probability of image \n",
    "    predictions = layers.Dense(len(training_class), activation='softmax')(x)\n",
    "\n",
    "    base_model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    #base_model.summary()\n",
    "    \n",
    "    finetuned_model, best_learnetweights= Training_model(base_model,train_generator,validation_generator, training_class, \n",
    "                                                         folder, index, Process )  #, Process='Pretrained_model'\n",
    "    \n",
    "    return finetuned_model, best_learnetweights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b4640d",
   "metadata": {},
   "source": [
    "### ASG Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bc95a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function implements the ASG process which utilises the curriculum learning strategy\n",
    "from ascending to descending order with one single iteration. \n",
    "\"\"\"\n",
    "\n",
    "def ASG(granularity,index,Level, delta, Process):   \n",
    "    \n",
    "    \n",
    "    finetuned_model, best_learnetweights= ImageNet_model(Dataset_path, granularity,index, Level, Process)   \n",
    "       \n",
    "    for i in range(len(granularity)-1):       \n",
    "        \n",
    "        Level= Level+ delta   \n",
    "        next_level= granularity[Level] \n",
    "        next_index=index[Level]   \n",
    "        print('level=',Level,'g=',i,'next_level= ',next_level,'next_index=',next_index)\n",
    "        granularity_path= os.path.join(Dataset_path, next_level)\n",
    "        train_generator, validation_generator, training_class = image_datagenerator(granularity_path)\n",
    "        \n",
    "        finetuned_model, best_learnetweights= TransferLearning(finetuned_model,best_learnetweights, train_generator,\n",
    "                                                         validation_generator, training_class, \n",
    "                                                          next_level, next_index, Process) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bbba5d",
   "metadata": {},
   "source": [
    "### DEG Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5020fcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function implements the DEG process from descending to ascending order with one single iteration. It takes in a list of granularity levels,\n",
    "an index, the current level, a delta value, and the process type. The function follows these steps:\n",
    "\n",
    "1. Fine-tunes the model using a pretrained ImageNet model.\n",
    "2. Iteratively moves through the granularity levels, adjusting the level and index based on the delta value.\n",
    "3. For each new level, the function loads the corresponding granularity dataset, applies transfer learning, \n",
    "   and updates the model weights.\n",
    "\n",
    "Returns:\n",
    "- finetuned_model: The model after transfer learning.\n",
    "- best_learnetweights: The best model weights found during training.\n",
    "\"\"\"\n",
    "\n",
    "def DEG(granularity,index,Level, delta, Process):   # granularity=[gmax,g_4,...,g_1], level=0, delta=1, index=5\n",
    "    \n",
    "    \n",
    "    finetuned_model, best_learnetweights= ImageNet_model(Dataset_path, granularity,index, Level, Process)   \n",
    "       \n",
    "    for i in range(len(granularity)-1):       \n",
    "        \n",
    "        Level= Level+ delta   \n",
    "        next_level= granularity[Level] \n",
    "        next_index=index[Level]   \n",
    "        print('level=',Level,'g=',i,'next_level= ',next_level,'next_index=',next_index)\n",
    "        granularity_path= os.path.join(Dataset_path, next_level)\n",
    "        train_generator, validation_generator, training_class = image_datagenerator(granularity_path)\n",
    "        \n",
    "        finetuned_model, best_learnetweights= TransferLearning(finetuned_model,best_learnetweights, train_generator,\n",
    "                                                         validation_generator, training_class, \n",
    "                                                          next_level, next_index, Process) \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff659c7",
   "metadata": {},
   "source": [
    "### CLOG-CD based on different oscillation steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6c19c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function implements the CLOG-CD model over I iterations with different oscillation steps.\n",
    "Returns:\n",
    "- finetuned_model: The model after transfer learning.\n",
    "- best_learnetweights: The best model weights found during training.\n",
    "\"\"\"\n",
    "\n",
    "def CLOG_CD(granularity,index,Level, delta, Process):\n",
    "    finetuned_model, best_learnetweights= ImageNet_model(Dataset_path, granularity, index, Level, Process)\n",
    "    \n",
    "    num_iter= 0\n",
    "    # Loop through the granularity levels for 20 iterations.   \n",
    "    while num_iter < 20:\n",
    "        \n",
    "        num_iter+=1\n",
    "        \n",
    "        for i in range(len(granularity)-1):        #[g_4,...,g_1]    \n",
    "            \n",
    "           # ascending_order= granularity\n",
    "            path_iter = os.path.join(save_to_dir,Process, str(num_iter))\n",
    "            if not os.path.exists(path_iter):\n",
    "                os.makedirs(path_iter)\n",
    "            \n",
    "            \n",
    "            Level= Level+ delta            \n",
    "            next_level= granularity[Level]  \n",
    "            next_index=index[Level]        \n",
    "            print('level=',Level,'g=',i,'next_level= ',next_level,'next_index=',next_index)\n",
    "            granularity_path= os.path.join(Dataset_path, next_level)\n",
    "            \n",
    "            train_generator, validation_generator, training_class = image_datagenerator(granularity_path)\n",
    "        \n",
    "            finetuned_model, best_learnetweights= TransferLearning(finetuned_model,best_learnetweights, train_generator,\n",
    "                                                         validation_generator, training_class, \n",
    "                                                          next_level, next_index, Process=path_iter)  \n",
    "            \n",
    "\n",
    "            \n",
    "        # apply the backward direction   \n",
    "        beta=Level   # backward from mininum granularity to maximum direction\n",
    "        num_iter+=1\n",
    "        \n",
    "        for j in range(len(granularity)-1):        #[g_4,...,g_1]\n",
    "                \n",
    "            path_iter = os.path.join(save_to_dir,Process, str(num_iter))\n",
    "            if not os.path.exists(path_iter):\n",
    "                os.makedirs(path_iter)\n",
    "                \n",
    "                \n",
    "            beta= beta- delta    #beta=4-1 = 3\n",
    "            backward_level= granularity[beta]  # granularity[3]=g_2\n",
    "            backward_index=index[beta]         # index[3] = 2\n",
    "            print('beta=',beta,'g=',j,'backward_level= ',backward_level,'backward_index=',backward_index)\n",
    "            \n",
    "            granularity_path= os.path.join(Dataset_path, backward_level)\n",
    "            train_generator, validation_generator, training_class = image_datagenerator(granularity_path)\n",
    "        \n",
    "            finetuned_model, best_learnetweights= TransferLearning(finetuned_model,best_learnetweights, train_generator,\n",
    "                                                         validation_generator, training_class, \n",
    "                                                          backward_level, backward_index, Process=path_iter) \n",
    "            Level=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d55a21c",
   "metadata": {},
   "source": [
    "### Transfer the learnt weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b93cb74d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T12:23:51.491478Z",
     "iopub.status.busy": "2023-04-30T12:23:51.490742Z",
     "iopub.status.idle": "2023-04-30T12:23:51.499751Z",
     "shell.execute_reply": "2023-04-30T12:23:51.498515Z",
     "shell.execute_reply.started": "2023-04-30T12:23:51.491435Z"
    }
   },
   "outputs": [],
   "source": [
    "def TransferLearning(TransfereLearned_Model, best_learnetweights, train_generator, validation_generator, \n",
    "                      training_class, next_level, index,  Process):       \n",
    "    \n",
    "\n",
    "    TransfereLearned_Model.load_weights(best_learnetweights)\n",
    "    \n",
    "    TransfereLearned_Model = Model(TransfereLearned_Model.input, TransfereLearned_Model.layers[-2].output) #cut off the last layer\n",
    "\n",
    "              \n",
    "# Add Dense layer\n",
    "#adding the new classification output layer corresponding to the new downstream task\n",
    "    new_prediction =layers.Dense(len(training_class), activation='softmax', name=\"new_task\")(TransfereLearned_Model.output)\n",
    "    \n",
    "    \n",
    "# build the 4S_DT model and visualize it\n",
    "    TransfereLearned_Model = Model(inputs=TransfereLearned_Model.input, outputs=new_prediction)\n",
    "\n",
    "    #folder= Level\n",
    "    finetuned_model, best_learnetweights = Training_model(TransfereLearned_Model,train_generator,validation_generator, \n",
    "                                                    training_class, next_level, index, Process)\n",
    "    \n",
    "   \n",
    "    return finetuned_model, best_learnetweights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f01e733",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T12:23:54.498846Z",
     "iopub.status.busy": "2023-04-30T12:23:54.498456Z",
     "iopub.status.idle": "2023-04-30T12:23:54.512314Z",
     "shell.execute_reply": "2023-04-30T12:23:54.510741Z",
     "shell.execute_reply.started": "2023-04-30T12:23:54.498810Z"
    }
   },
   "outputs": [],
   "source": [
    "def Training_model(finetuning_model, train_generator, validation_generator, training_class, folder, index, Process):\n",
    "\n",
    "    save_here = os.path.join(save_to_dir, Process)\n",
    "    \n",
    "    if not os.path.exists(save_here):\n",
    "            os.makedirs(save_here)\n",
    "            \n",
    "    #name=os.path.basename(os.path.normpath(trainpath))\n",
    "    best_learnetweights = os.path.join(save_here,'weights_'+folder+'.h5')     #creat folders based on the last name\n",
    "   \n",
    "    #define checkpoint\n",
    "    checkpoint = ModelCheckpoint(filepath= best_learnetweights,\n",
    "                                 monitor='val_accuracy',save_best_only=True,\n",
    "                                 save_weights_only=True, \n",
    "                                 mode='max', verbose=1)      \n",
    "    #early stopping\n",
    "    earlystop = EarlyStopping (monitor=\"val_accuracy\", \n",
    "                                patience=5,  \n",
    "                                mode=\"auto\")\n",
    "\n",
    "    def lr_scheduler(epoch):\n",
    "        initial_lrate = 0.001\n",
    "        drop = 0.85\n",
    "        epochs_drop = 10.0\n",
    "        lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "        return lrate\n",
    "  \n",
    "    lrscheduler = LearningRateScheduler(lr_scheduler)\n",
    "    callbacks = [checkpoint, earlystop, lrscheduler]\n",
    "\n",
    "    batch_size=5\n",
    "    \n",
    "    # Print the number of samples\n",
    "    print(f\"Number of training samples: {train_generator.samples}\")\n",
    "    print(f\"Number of validation samples: {validation_generator.samples}\")\n",
    "\n",
    "    # Calculate steps per epoch\n",
    "    steps_per_epoch = int(np.ceil(train_generator.samples / batch_size))\n",
    "    validation_steps = int(np.ceil(validation_generator.samples / batch_size))\n",
    "\n",
    "    # Print steps per epoch and validation steps\n",
    "    print(f\"Steps per epoch: {steps_per_epoch}\")\n",
    "    print(f\"Validation steps: {validation_steps}\")\n",
    "    \n",
    "    finetuning_model.compile( optimizer=SGD(), loss=\"mse\", metrics=[\"accuracy\"])\n",
    "\n",
    "    history=finetuning_model.fit (train_generator,\n",
    "                                    steps_per_epoch=steps_per_epoch,\n",
    "                                    validation_data=validation_generator,\n",
    "                                    validation_steps=validation_steps, epochs=50,\n",
    "                                    callbacks= callbacks, verbose=1, shuffle= True)\n",
    "    \n",
    " \n",
    "    visualize_results(history, save_here, folder)\n",
    "    \n",
    "    #######################################################################################\n",
    "    y_true, y_predict= model_prediction(finetuning_model, best_learnetweights, save_here)\n",
    "    \n",
    "    Evaluation.ConfusionMatrix (y_true, y_predict, save_here, folder, index)\n",
    "    \n",
    "    \n",
    "    return finetuning_model, best_learnetweights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d654d60a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T12:23:58.446599Z",
     "iopub.status.busy": "2023-04-30T12:23:58.445849Z",
     "iopub.status.idle": "2023-04-30T12:23:58.455003Z",
     "shell.execute_reply": "2023-04-30T12:23:58.453854Z",
     "shell.execute_reply.started": "2023-04-30T12:23:58.446558Z"
    }
   },
   "outputs": [],
   "source": [
    "# Utility function for plotting of the model results\n",
    "\n",
    "def visualize_results(history, save_here,folder_name):\n",
    "    # Plot the accuracy and loss curves\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    epochs = range(len(acc))\n",
    "\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(epochs, acc, 'b', label='Training acc')\n",
    "    plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Training and validation Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Training and validation Loss')\n",
    " \n",
    "    # save the figure\n",
    "\n",
    "    plt.savefig(save_here+'LearningCurve_'+ folder_name+'.png')\n",
    "    plt.savefig(os.path.join(save_here, 'LearningCurve_'+folder_name+'.png'))\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80ab413",
   "metadata": {},
   "source": [
    "###  Make a prediction on a test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3519af4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T12:24:03.527154Z",
     "iopub.status.busy": "2023-04-30T12:24:03.526422Z",
     "iopub.status.idle": "2023-04-30T12:24:03.534457Z",
     "shell.execute_reply": "2023-04-30T12:24:03.533373Z",
     "shell.execute_reply.started": "2023-04-30T12:24:03.527114Z"
    }
   },
   "outputs": [],
   "source": [
    "def model_prediction( model, best_learnetweights, save_here, batch_size=1):\n",
    "    x_test , y_test = [] , []\n",
    "    for i in range(test_generator.n//1):\n",
    "        a , b = test_generator.next()\n",
    "        x_test.extend(a) \n",
    "        y_test.extend(b)\n",
    "    y_test= np.array(y_test)\n",
    "    \n",
    "    # Predict the output\n",
    "    STEP_SIZE_TEST=test_generator.n//test_generator.batch_size\n",
    "    test_generator.reset()\n",
    "    \n",
    "    \n",
    "    # loading the convergence weights \n",
    "    model.load_weights(os.path.join(save_here,best_learnetweights) )       \n",
    "    #make prediction\n",
    "    print('Make a prediction on a test set:')\n",
    "    y_test_pred= model.predict(test_generator,steps=STEP_SIZE_TEST,verbose=1)\n",
    "    y_prediction = np.argmax(y_test_pred, axis=1)## predicted_class_indices\n",
    "     \n",
    "    # ground truth labels\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "    return y_true, y_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84688a3c",
   "metadata": {},
   "source": [
    "### Evaluate the model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e2374b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T12:24:10.140342Z",
     "iopub.status.busy": "2023-04-30T12:24:10.139256Z",
     "iopub.status.idle": "2023-04-30T12:24:10.156691Z",
     "shell.execute_reply": "2023-04-30T12:24:10.155468Z",
     "shell.execute_reply.started": "2023-04-30T12:24:10.140282Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This class handles the evaluation of a classification model, including generating the confusion matrix,\n",
    "computing accuracy, precision, recall, and F1 score.\n",
    "\n",
    "Attributes:\n",
    "- test_classes: List of class names used in the classification task.\n",
    "- y_predict: Array of predicted labels.\n",
    "- y_true: Array of true labels.\n",
    "- folder_name: Identifier for the current experiment or granularity level.\n",
    "- index: Correction factor applied to predictions when needed.\n",
    "- save_to_dir: Directory where evaluation results will be saved.\n",
    "\"\"\"\n",
    "\n",
    "class Evaluation:\n",
    "    def __int__(self, test_classes, y_predict, y_true, folder_name, index, save_here):\n",
    "        self.test_classes= test_classes\n",
    "        self.y_predict= y_predict\n",
    "        self.y_true= y_true\n",
    "        self.folder_name= folder_name\n",
    "        self.index= index\n",
    "        self.save_to_dir= save_to_dir\n",
    "        \n",
    "    \n",
    "    \n",
    "    def ConfusionMatrix (y_true, y_predict,save_here, folder_name, index):       #get confusion matrix\n",
    "        \n",
    "        if folder_name !='k_1':        ###  Refine the final classification using error-correction criteria. \n",
    "            correct_prediction=[]\n",
    "            for i in y_predict:\n",
    "                correct_prediction.append(i // index)            \n",
    "            y_predict=np.array(correct_prediction)\n",
    " \n",
    "\n",
    "        #get confusion matrix\n",
    "        cm = confusion_matrix(y_true, y_predict)\n",
    "        #plot\n",
    "        fig, ax = plot_confusion_matrix(conf_mat=cm,  figsize=(6, 6),\n",
    "                                colorbar=False,\n",
    "                                show_absolute=True,\n",
    "                                show_normed=False,\n",
    "                                class_names=test_classes,cmap=\"Blues\")\n",
    "        # save the figure\n",
    "        plt.savefig(os.path.join(save_here, 'ConfusionMatrix_'+ folder_name +'.png'))\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "        #get classification report\n",
    "        print(classification_report(y_true, y_predict, target_names = test_classes, digits=4))\n",
    "          \n",
    "        print('Overall accuracy= ', accuracy_score(y_true, y_predict))\n",
    "        #\n",
    "        #\n",
    "        #precision and recall for each class\n",
    "\n",
    "        tp = np.diag(cm)\n",
    "        prec = list(map(truediv, tp, np.sum(cm, axis=0)))\n",
    "        rec = list(map(truediv, tp, np.sum(cm, axis=1)))\n",
    "        print ('\\nPrecision: {}\\nRecall: {}'.format(prec, rec))\n",
    "\n",
    "        #print precision value of model\n",
    "        precision = np.diag(cm) / np.sum(cm, axis = 0)\n",
    "\n",
    "        #print recall value of model\n",
    "        recall = np.diag(cm) / np.sum(cm, axis = 1)\n",
    "\n",
    "        F1_score= 2*((precision*recall)/(precision + recall))\n",
    "        print('F1_score= ', F1_score)\n",
    "\n",
    "        print('================================================')\n",
    "        #To get overall measures of precision and recall, use then\n",
    "        PR=np.mean(precision)\n",
    "        RE= np.mean(recall)\n",
    "        print('overall_ Precision= ', PR)\n",
    "        print('overall_Recall= ', RE)\n",
    "\n",
    "\n",
    "        F1= 2*((PR* RE)/( PR + RE))\n",
    "        print('overall F1_score= ', F1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a5aaced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The decomposition granularity is:  ['g_1', 'g_2', 'g_3', 'g_4', 'g_5']\n",
      "The descending-ascending order is:  ['g_5', 'g_4', 'g_3', 'g_2', 'g_1']\n",
      "Index=  1   The granularity decomposition class is:  g_5\n",
      "Index=  2   The granularity decomposition class is:  g_4\n",
      "Index=  3   The granularity decomposition class is:  g_3\n",
      "Index=  4   The granularity decomposition class is:  g_2\n",
      "Index=  5   The granularity decomposition class is:  g_1\n",
      "[5, 4, 3, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "# directory where the data is located\n",
    "\n",
    "Dataset_path= ('V:/CLOG_CD/brain/k_means - Copy/')  #data path\n",
    "save_to_dir=('V:/CLOG_CD/brain/New folder/')\n",
    "#Next_level= False\n",
    "\n",
    "#Determine the decomposition granularity\n",
    "decomposition_granularity = os.listdir(Dataset_path)    # G=[g1,g2,...,9_max]  \n",
    "print('The decomposition granularity is: ', decomposition_granularity )\n",
    "\n",
    "\n",
    "'''\n",
    "In our work, the CLOG_CD model starts training from the high-level granularity (g=5), then move towards\n",
    "the lower-level granularity (g=1)\n",
    "\n",
    "'''\n",
    "\n",
    "# sort array with descending-ascending order\n",
    "\n",
    "g=[]\n",
    "index=[]\n",
    "decomposition_granularity.sort(reverse=True)\n",
    "\n",
    "\n",
    "print('The descending-ascending order is: ', decomposition_granularity )\n",
    "\n",
    "for i, folder in list(enumerate(decomposition_granularity,1)):         # G=[9_max,...,g2,g1]\n",
    "    print('Index= ',i, '  The granularity decomposition class is: ',folder)\n",
    "    g.append(folder)     # if max=5 then: G=[9_5, g_4, g_3, g_2, g_1]\n",
    "    index.append(i)      # if max=5 then:  index=[5, 4, 3, 2, 1]\n",
    "    \n",
    "index.sort(reverse=True)\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acb0a9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CLOG_CD = The_proccess()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa9333f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6005b43e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f801aa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bfa5b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
